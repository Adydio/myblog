---
title: 线性回归与梯度下降
date: 2023-01-17 23:32:04
tags:
- 机器学习
mathjax: true
---

# 线性回归与梯度下降

#### Linear Regression

- The model function for linear regression, which is a function that maps from `x`  to `y`  is represented as $f_{w,b}(x) = wx + b$.

- To train a linear regression model, you want to find the best $(w,b)$ parameters that fit your dataset.  

  - To compare how one choice of $(w,b)$ is better or worse than another choice, you can evaluate it with a cost function $J(w,b)$
    - $J$ is a function of $(w,b)$. That is, the value of the cost $J(w,b)$ depends on the value of $(w,b)$.

  - The choice of $(w,b)$ that fits your data the best is the one that has the smallest cost $J(w,b)$.


- To find the values $(w,b)$ that gets the smallest possible cost $J(w,b)$, you can use a method called **gradient descent**. 
  - With each step of gradient descent, your parameters $(w,b)$ come closer to the optimal values that will achieve the lowest cost $J(w,b)$.

- The trained linear regression model can then take the input feature $x$ and output a prediction $f_{w,b}(x)$.

#### Gradient Descent

Gradient descent involves repeated steps to adjust the value of your parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$.

- At each step of gradient descent, it will be helpful for you to monitor your progress by computing the cost $J(w,b)$ as $(w,b)$ gets updated. 
- In this section, you will implement a function to calculate $J(w,b)$ so that you can check the progress of your gradient descent implementation.

###### Cost function

As you may recall from the lecture, for one variable, the cost function for linear regression $J(w,b)$ is defined as

$$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ 

- You can think of $f_{w,b}(x^{(i)})$ as the model's prediction of your restaurant's profit, as opposed to $y^{(i)}$, which is the actual profit that is recorded in the data.
- $m$ is the number of training examples in the dataset

###### Model prediction

- For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is represented as $ f_{w,b}(x^{(i)}) = wx^{(i)} + b$.

This is the equation for a line, with an intercept $b$ and a slope $w$.
